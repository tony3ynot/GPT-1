{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNZaeZhWxpgB2UPJrrPv7uz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tony3ynot/GPT-1/blob/main/GPT_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcew6U8EFR19"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from einops import rearrange"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "wJHNosOQMJbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Model Architecture\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Izu7pwD4YjAM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1-1. Transformer Decoder"
      ],
      "metadata": {
        "id": "RGakpsNKaXXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Multi-Head Attention\n",
        "class MHA(nn.Module):\n",
        "    def __init__(self, d_model, n_heads):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        self.fc_q = nn.Linear(d_model, d_model) # Query\n",
        "        self.fc_k = nn.Linear(d_model, d_model) # Key\n",
        "        self.fc_v = nn.Linear(d_model, d_model) # Value\n",
        "\n",
        "        self.fc = nn.Linear(d_model, d_model) # Linear Layer\n",
        "\n",
        "        self.scale = torch.sqrt(torch.tensor(d_model/n_heads))\n",
        "\n",
        "    def forward(self, Q, K, V, mask = None):\n",
        "        Q = self.fc_q(Q)\n",
        "        K = self.fc_k(K)\n",
        "        V = self.fc_v(V)\n",
        "\n",
        "        ## B = batch size / L = length / H = heads / D = dimension\n",
        "        # rearrange to implement 'heads'\n",
        "        Q = rearrange(Q, 'B L (H D) -> B H L D', H = self.n_heads)\n",
        "        K = rearrange(K, 'B L (H D) -> B H L D', H = self.n_heads)\n",
        "        V = rearrange(V, 'B L (H D) -> B H L D', H = self.n_heads)\n",
        "\n",
        "        ## Self-Attention\n",
        "        # 1. MatMul\n",
        "        attention_score = Q @ K.transpose(-2, -1)\n",
        "\n",
        "        # 2. Scale\n",
        "        attention_score = attention_score / self.scale\n",
        "\n",
        "        # 3. Masking\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1)\n",
        "            attention_score.masked_fill_(mask, -1e9)\n",
        "\n",
        "        # 4. SoftMax\n",
        "        attention_weights = torch.softmax(attention_score, dim=-1)\n",
        "\n",
        "        # 5. MatMul\n",
        "        attention = attention_weights @ V\n",
        "\n",
        "        ## Concat & Linear\n",
        "        # rearrange to concat\n",
        "        x = rearrange(attention, 'B H L D -> B L (H D)')\n",
        "        output = self.fc(x)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "### Feed Forward Network\n",
        "class FFN(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super().__init__()\n",
        "\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.gelu = nn.GELU()\n",
        "\n",
        "        nn.init.xavier_normal_(self.linear1.weight)\n",
        "        nn.init.xavier_normal_(self.linear2.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.gelu(self.linear1(x))\n",
        "        output = self.linear2(x)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "### Decoder Layer\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, resid_drop):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mha = MHA(d_model, n_heads)\n",
        "        self.dropout1 = nn.Dropout(resid_drop)\n",
        "        self.layernorm1 = nn.LayerNorm(d_model, eps=1e-5)\n",
        "\n",
        "        self.ffn = FFN(d_model, d_ff)\n",
        "        self.dropout2 = nn.Dropout(resid_drop)\n",
        "        self.layernorm2 = nn.LayerNorm(d_model, eps=1e-5)\n",
        "\n",
        "    def forward(self, x, attn_mask):\n",
        "        # Masked-MHA layer (with residual shortcut connection)\n",
        "        residual = self.mha(x, x, x, attn_mask)\n",
        "        residual = self.dropout1(residual)\n",
        "        x = self.layernorm1(x + residual)\n",
        "\n",
        "        # FFN layer (with residual shortcut connection)\n",
        "        residual = self.ffn(x)\n",
        "        residual = self.dropout2(residual)\n",
        "        output = self.layernorm2(x + residual)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "### Decoder\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, seq_len, d_model, n_layers, n_heads, d_ff, embd_drop, resid_drop, pad_id):\n",
        "        super().__init__()\n",
        "\n",
        "        self.pad_id = pad_id\n",
        "\n",
        "        ## Decoder Input\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.dropout = nn.Dropout(embd_drop)\n",
        "        self.pos_embedding = nn.Embedding(seq_len+1, d_model) # learned positional embedding\n",
        "\n",
        "        ## Decoder Layers\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, resid_drop) for _ in range(n_layers)])\n",
        "\n",
        "        nn.init.xavier_normal_(self.embedding.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ## padding mask for position embedding\n",
        "        positions = torch.arange(x.size(1), device=x.device).repeat(x.size(0), 1) + 1\n",
        "        position_pad_mask = x.eq(self.pad_id)\n",
        "        positions.masked_fill_(position_pad_mask, 0)\n",
        "\n",
        "        output = self.dropout(self.embedding(x)) + self.pos_embedding(positions)\n",
        "\n",
        "        ## attention mask\n",
        "        pad_mask = self.get_padding_mask(x, x, self.pad_id)\n",
        "        future_mask = self.get_future_mask(x).to(device=pad_mask.device)\n",
        "        attn_mask = torch.gt((pad_mask.to(dtype=future_mask.dtype) + future_mask), 0)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            output = layer(output, attn_mask)\n",
        "\n",
        "        return output\n",
        "\n",
        "    ## padding mask : apply masking to padding tokens\n",
        "    def get_padding_mask(self, q, k, pad_id):\n",
        "        pad_mask = k.eq(pad_id).unsqueeze(1).repeat(1, q.size(1), 1)\n",
        "\n",
        "        return pad_mask\n",
        "\n",
        "    ## future token mask : apply masking to future tokens\n",
        "    def get_future_mask(self, q):\n",
        "        bs, q_len = q.size()\n",
        "        future_mask = torch.ones(bs, q_len, q_len).triu(diagonal=1)\n",
        "\n",
        "        return future_mask"
      ],
      "metadata": {
        "id": "wc6lljC2HVyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1-2. GPT-1"
      ],
      "metadata": {
        "id": "NUOohiy4agOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### GPT-1\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size,\n",
        "                 seq_len = 512,\n",
        "                 d_model = 768,\n",
        "                 n_layers = 12,\n",
        "                 n_heads = 12,\n",
        "                 d_ff = 3072,\n",
        "                 embd_drop = 0.1,\n",
        "                 resid_drop = 0.1,\n",
        "                 pad_id = 0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.decoder = TransformerDecoder(vocab_size, seq_len, d_model, n_layers, n_heads,\n",
        "                                          d_ff, embd_drop, resid_drop, pad_id)\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = self.decoder(x)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "### Language Model (pre-training)\n",
        "class GPTLMHead(nn.Module):\n",
        "    def __init__(self, gpt):\n",
        "        super().__init__()\n",
        "        vocab_size, d_model = gpt.decoder.embedding.weight.size()\n",
        "\n",
        "        self.gpt = gpt\n",
        "        self.linear = nn.Linear(d_model, vocab_size, bias = False)\n",
        "        self.linear.weight = gpt.decoder.embedding.weight\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.gpt(x)\n",
        "\n",
        "        lm_logits = self.linear(x)\n",
        "\n",
        "        return lm_logits\n",
        "\n",
        "\n",
        "### Classification Model (fine-tuning)\n",
        "class GPTClsHead(nn.Module):\n",
        "    def __init__(self, gpt, n_class, cls_token_id, cls_drop=0.1):\n",
        "        super().__init__()\n",
        "        vocab_size, d_model = gpt.decoder.embedding.weight.size()\n",
        "        self.cls_token_id = cls_token_id\n",
        "\n",
        "        self.gpt = gpt\n",
        "\n",
        "        # LM\n",
        "        self.linear1 = nn.Linear(d_model, vocab_size, bias=False)\n",
        "        self.linear1.weight = gpt.decoder.embedding.weight\n",
        "        # Cls\n",
        "        self.linear2 = nn.Linear(d_model, n_class)\n",
        "        self.dropout = nn.Dropout(cls_drop)\n",
        "\n",
        "        nn.init.normal_(self.linear2.weight, std=0.02)\n",
        "        nn.init.normal_(self.linear2.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = self.gpt(x)\n",
        "\n",
        "        lm_logits = self.linear1(outputs)\n",
        "\n",
        "        outputs = outputs[x.eq(self.cls_token_id)]\n",
        "        cls_logits = self.linear2(self.dropout(outputs))\n",
        "\n",
        "        return lm_logits, cls_logits"
      ],
      "metadata": {
        "id": "ORwuPmc2n5SA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Training"
      ],
      "metadata": {
        "id": "uM3QWuITiogM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2-1. Pre-training"
      ],
      "metadata": {
        "id": "aPLp-rTui5gP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets tokenizers\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "s5p6YOne8Dbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### WikiText Dataset class\n",
        "class WikiTextDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, seq_len):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.data[idx]['text']\n",
        "        encoded = self.tokenizer.encode(text)\n",
        "        input_ids = encoded.ids\n",
        "\n",
        "        # sequence length matching\n",
        "        if len(input_ids) > self.seq_len:\n",
        "            input_ids = input_ids[:self.seq_len]\n",
        "        else:\n",
        "            input_ids = input_ids + [0] * (self.seq_len - len(input_ids))\n",
        "\n",
        "        # input & target (for next-word prediction)\n",
        "        inputs = torch.tensor(input_ids[:-1])\n",
        "        targets = torch.tensor(input_ids[1:])\n",
        "\n",
        "        return inputs, targets"
      ],
      "metadata": {
        "id": "TZA257XGli8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Hyper-parameters\n",
        "VOCAB_SIZE = 10000\n",
        "SEQ_LEN = 512\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 3\n",
        "LEARNING_RATE = 1e-4\n",
        "\n",
        "### Tokenizer Training\n",
        "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
        "tokenizer = Tokenizer(BPE())\n",
        "trainer = BpeTrainer(vocab_size=VOCAB_SIZE, special_tokens=[\"<pad>\", \"<cls>\"])\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "def get_training_corpus():\n",
        "    for i in range(0, len(dataset['train'])):\n",
        "        yield dataset['train'][i]['text']\n",
        "\n",
        "tokenizer.train_from_iterator(get_training_corpus(), trainer)\n",
        "\n",
        "### Dataset Setup\n",
        "train_dataset = WikiTextDataset(dataset['train'], tokenizer, SEQ_LEN + 1)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "-HZULGGboWcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Model Initialization\n",
        "model = GPTLMHead(GPT(vocab_size=VOCAB_SIZE, seq_len=SEQ_LEN)).to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_dataloader) * EPOCHS)"
      ],
      "metadata": {
        "id": "2uzdYft7pzXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Pre-Training\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    progress_bar = tqdm(train_dataloader, desc=f'Epoch {epoch+1}/{EPOCHS}')\n",
        "    for batch_idx, (inputs, targets) in enumerate(progress_bar):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        logits = model(inputs)\n",
        "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=0)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        progress_bar.set_postfix({'loss': total_loss / (batch_idx + 1)})\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"\\nEpoch {epoch+1} Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "print(\"Training completed!\")\n",
        "\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'scheduler_state_dict': scheduler.state_dict(),\n",
        "    'final_loss': avg_loss\n",
        "}, 'gpt1_pretrained.pt')"
      ],
      "metadata": {
        "id": "bNvOeC9trDwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2-2. Fine-tuning"
      ],
      "metadata": {
        "id": "fOa4WPjIi9m_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### IMDB Dataset class\n",
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, seq_len):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.data[idx]['text']\n",
        "        label = self.data[idx]['label']\n",
        "\n",
        "        # Add CLS token at the start\n",
        "        encoded = self.tokenizer.encode(\"<cls> \" + text)\n",
        "        input_ids = encoded.ids\n",
        "\n",
        "        # Truncate or pad sequence\n",
        "        if len(input_ids) > self.seq_len:\n",
        "            input_ids = input_ids[:self.seq_len]\n",
        "        else:\n",
        "            input_ids = input_ids + [0] * (self.seq_len - len(input_ids))\n",
        "\n",
        "        return torch.tensor(input_ids), torch.tensor(label)"
      ],
      "metadata": {
        "id": "IqHdcsJ-rDNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Dataset Setup\n",
        "from datasets import load_dataset\n",
        "imdb_dataset = load_dataset('imdb')\n",
        "\n",
        "train_dataset = IMDBDataset(imdb_dataset['train'], tokenizer, SEQ_LEN)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "val_dataset = IMDBDataset(imdb_dataset['test'], tokenizer, SEQ_LEN)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=4)"
      ],
      "metadata": {
        "id": "7hwz9aLQjAbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "premodel = GPTLMHead(GPT(vocab_size=VOCAB_SIZE, seq_len=SEQ_LEN)).to(device)\n",
        "premodel.load_state_dict(torch.load('gpt1_pretrained.pt'))\n",
        "\n",
        "### Fine-tuning Model Initialization\n",
        "model = GPTClsHead(\n",
        "    gpt=premodel.gpt,  # pretrained GPT\n",
        "    n_class=2,\n",
        "    cls_token_id=tokenizer.token_to_id(\"<cls>\"),\n",
        "    cls_drop=0.1\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)"
      ],
      "metadata": {
        "id": "xsH1i14Wno67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Fine-tuning\n",
        "EPOCHS = 3\n",
        "auxiliary_ratio = 0.5\n",
        "best_acc = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    ## Training\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    progress_bar = tqdm(train_dataloader, desc=f'Epoch {epoch+1}/{EPOCHS}')\n",
        "    for batch_idx, (inputs, labels) in enumerate(progress_bar):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        lm_logits, cls_logits = model(inputs)\n",
        "        lm_logits = lm_logits[:, :-1].contiguous()\n",
        "\n",
        "        ## Loss Function w/ Auxiliary Function\n",
        "        lm_loss = F.cross_entropy(lm_logits.view(-1, lm_logits.size(-1)),\n",
        "                                  inputs[:, 1:].contiguous().view(-1), ignore_index=0) # L1 (Auxiliary)\n",
        "        cls_loss = F.cross_entropy(cls_logits, labels) # L2\n",
        "        loss = cls_loss + (auxiliary_ratio * lm_loss) # L3\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        progress_bar.set_postfix({'loss': total_loss / (batch_idx + 1)})\n",
        "\n",
        "    ## Validation\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            _, cls_logits = model(inputs)\n",
        "\n",
        "            predictions = torch.argmax(cls_logits, dim=-1)\n",
        "            correct += (predictions == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f\"Epoch {epoch+1} Validation Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Save best model\n",
        "    if accuracy > best_acc:\n",
        "        best_acc = accuracy\n",
        "        torch.save(model.state_dict(), 'gpt1_imdb_best.pt')\n",
        "\n",
        "print(f\"Fine-tuning completed! Best accuracy: {best_acc:.4f}\")"
      ],
      "metadata": {
        "id": "L3xUxig9n1ft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "RzSJJXmroraC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPTClsHead(\n",
        "    GPT(vocab_size=VOCAB_SIZE, seq_len=SEQ_LEN),\n",
        "    n_class=2,\n",
        "    cls_token_id=tokenizer.token_to_id(\"<cls>\"),\n",
        "    cls_drop=0.1\n",
        ")\n",
        "\n",
        "model.load_state_dict(torch.load('gpt1_imdb_best.pt'))"
      ],
      "metadata": {
        "id": "eiYZh_pUzRS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "def predict_sentiment(text):\n",
        "    model.eval()\n",
        "    encoded = tokenizer.encode(\"<cls> \" + text)\n",
        "    input_ids = encoded.ids\n",
        "\n",
        "    if len(input_ids) > SEQ_LEN:\n",
        "        input_ids = input_ids[:SEQ_LEN]\n",
        "    else:\n",
        "        input_ids = input_ids + [0] * (SEQ_LEN - len(input_ids))\n",
        "\n",
        "    inputs = torch.tensor([input_ids]).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _, cls_logits = model(inputs)\n",
        "        prediction = torch.argmax(cls_logits, dim=-1)\n",
        "\n",
        "    return \"Positive\" if prediction.item() == 1 else \"Negative\"\n",
        "\n",
        "# Example\n",
        "test_text = \"This movie was really great! I enjoyed every moment of it.\"\n",
        "print(f\"Text: {test_text}\")\n",
        "print(f\"Sentiment: {predict_sentiment(test_text)}\")"
      ],
      "metadata": {
        "id": "pcNf8QL1otnb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}