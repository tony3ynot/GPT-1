{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKpzeKqTvrBoAiJhYewkEn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tony3ynot/GPT-1/blob/main/GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcew6U8EFR19"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJHNosOQMJbS",
        "outputId": "a3dbcfd8-1239-482f-d0ba-1a3483cd13cb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, d_k, attn_drop):\n",
        "        super().__init__()\n",
        "        self.d_k = d_k\n",
        "        self.dropout = nn.Dropout(attn_drop)\n",
        "\n",
        "    def forward(self, q, k, v, attn_mask):\n",
        "        attn_score = torch.matmul(q, k.transpose(-1, -2)) / (self.d_k ** 0.5)\n",
        "        attn_score.masked_fill_(attn_mask, -1e9)\n",
        "\n",
        "        attn_weights = nn.Softmax(dim=-1)(attn_score)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        output = torch.matmul(attn_weights, v)\n",
        "\n",
        "        return output, attn_weights\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, attn_drop):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = self.d_v = d_model//n_heads\n",
        "\n",
        "        self.WQ = nn.Linear(d_model, d_model)\n",
        "        self.WK = nn.Linear(d_model, d_model)\n",
        "        self.WV = nn.Linear(d_model, d_model)\n",
        "        self.scaled_dot_product_attn = ScaledDotProductAttention(self.d_k, attn_drop)\n",
        "        self.linear = nn.Linear(n_heads * self.d_v, d_model)\n",
        "\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        batch_size = Q.size(0)\n",
        "\n",
        "        q_heads = self.WQ(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        k_heads = self.WK(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        v_heads = self.WV(V).view(batch_size, -1, self.n_heads, self.d_v).transpose(1, 2)\n",
        "\n",
        "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1)\n",
        "        attn, attn_weights = self.scaled_dot_product_attn(q_heads, k_heads, v_heads, attn_mask)\n",
        "\n",
        "        attn = attn.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_v)\n",
        "        outputs = self.Linear(attn)\n",
        "\n",
        "        return outputs, attn_weights\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super().__init__()\n",
        "\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.gelu = nn.GELU()\n",
        "\n",
        "        nn.init.normal__(self.linear1.weight, std=0.02)\n",
        "        nn.init.normal__(self.linear2.weight, std=0.02)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        outputs = self.gelu(self.linear1(inputs))\n",
        "        outputs = self.linear2(outputs)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, attn_drop, resid_drop):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, n_heads, attn_drop)\n",
        "        self.dropout1 = nn.Dropout(resid_drop)\n",
        "        self.layernorm1 = nn.LayerNorm(d_model, eps=1e-5)\n",
        "\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        self.dropout2 = nn.Dropout(resid_drop)\n",
        "        self.layernorm2 = nn.LayerNorm(d_model, eps=1e-5)\n",
        "\n",
        "    def forward(self, inputs, attn_mask):\n",
        "        attn_outputs, attn_weights = self.mha(inputs, inputs, inputs, attn_mask)\n",
        "        attn_outputs = self.dropout1(attn_outputs)\n",
        "        attn_outputs = self.layernorm1(inputs + attn_outputs)\n",
        "\n",
        "        ffn_outputs = self.ffn(attn_outputs)\n",
        "        ffn_outputs = self.dropout2(ffn_outputs)\n",
        "        ffn_outputs = self.layernorm2(attn_outputs + ffn_outputs)\n",
        "\n",
        "        return ffn_outputs, attn_weights\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, seq_len, d_model, n_layers, n_heads, d_ff, embd_drop, attn_drop, resid_drop, pad_id):\n",
        "        super().__init__()\n",
        "        self.pad_id = pad_id\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.dropout = nn.Dropout(embd_drop)\n",
        "        self.pos_embedding = nn.Embedding(seq_len+1, d_model)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, attn_drop, resid_drop) for _ in range(n_layers)])\n",
        "\n",
        "        nn.init.normal_(self.embedding.weight, std=0.02)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        positions = torch.arange(inputs.size(1), device=inputs.device, dtype=inputs.dtype).repeat(inputs.size(0), 1) + 1\n",
        "        position_pad_mask = inputs.eq(self.pad_id)\n",
        "        positions.masked_fill_(position_pad_mask, 0)\n",
        "\n",
        "        outputs = self.dropout(self.embedding(inputs)) + self.pos_embedding(positions)\n",
        "\n",
        "        attn_pad_mask = self.get_attention_padding_mask(inputs, inputs, self.pad_id)\n",
        "        subsequent_mask = self.get_attention_subsequent_mask(inputs).to(device=attn_pad_mask.device)\n",
        "        attn_mask = torch.gt((attn_pad_mask.to(dtype=subsequent_mask.dtype) + subsequent_mask), 0)\n",
        "\n",
        "        attention_weights = []\n",
        "        for layer in self.layers:\n",
        "            outputs, attn_weights = layer(outputs, attn_mask)\n",
        "            attention_weights.append(attn_weights)\n",
        "\n",
        "        return outputs, attention_weights\n",
        "\n",
        "    def get_attention_padding_mask(self, q, k, pad_id):\n",
        "        attn_pad_mask = k.eq(pad_id).unsqueeze(1).repeat(1, q.size(1), 1)\n",
        "\n",
        "        return attn_pad_mask\n",
        "\n",
        "    def get_attention_subsequent_mask(self, q):\n",
        "        bs, q_len = q.size()\n",
        "        subsequent_mask = torch.ones(bs, q_len, q_len).triu(diagonal=1)\n",
        "\n",
        "        return subsequent_mask"
      ],
      "metadata": {
        "id": "wc6lljC2HVyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size,\n",
        "                 seq_len = 512,\n",
        "                 d_model = 768,\n",
        "                 n_layers = 12,\n",
        "                 n_heads = 12,\n",
        "                 d_ff = 3072,\n",
        "                 embd_drop = 0.1,\n",
        "                 attn_drop = 0.1,\n",
        "                 resid_drop = 0.1,\n",
        "                 pad_id = 0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.decoder = TransformerDecoder(vocab_size, seq_len, d_model, n_layers, n_heads,\n",
        "                                          d_ff, embd_drop, attn_drop, resid_drop, pad_id)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        outputs, attention_weights = self.decoder(inputs)\n",
        "\n",
        "        return outputs, attention_weights\n",
        "\n",
        "class GPTLMHead(nn.Module):\n",
        "    def __init__(self, gpt):\n",
        "        super().__init__()\n",
        "        vocab_size, d_model = gpt.decoder.embedding.weight.size()\n",
        "\n",
        "        self.gpt = gpt\n",
        "        self.linear = nn.Linear(d_model, vocab_size, bias = False)\n",
        "        self.linear.weight = gpt.decoder.embedding.weight\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        outputs, attention_weights = self.gpt(inputs)\n",
        "\n",
        "        lm_logits = self.linear(outputs)\n",
        "\n",
        "        return lm_logits\n",
        "\n",
        "class GPTClsHead(nn.Module):\n",
        "    def __init__(self, gpt, n_class, cls_token_id, cls_drop=0.1):\n",
        "        super().__init__()\n",
        "        vocab_size, d_model = gpt.decoder.embedding.weight.size()\n",
        "        self.cls_token_id = cls_token_id\n",
        "\n",
        "        self.gpt = gpt\n",
        "\n",
        "        # LM\n",
        "        self.linear1 = nn.Linear(d_model, vocab_size, bias=False)\n",
        "        self.linear1.weight = gpt.decoder.embedding.weight\n",
        "        # Cls\n",
        "        self.linear2 = nn.Linear(d_model, n_class)\n",
        "        self.dropout = nn.Dropout(cls_drop)\n",
        "\n",
        "        nn.init.normal_(self.linear2.weight, std=0.02)\n",
        "        nn.init.normal_(self.linear2.bias, 0)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        outputs, attention_weights = self.gpt(inputs)\n",
        "\n",
        "        lm_logits = self.linear1(outputs)\n",
        "\n",
        "        outputs = outputs[inputs.eq(self.cls_token_id)]\n",
        "        cls_logits = self.linear2(self.dropout(outputs))\n",
        "\n",
        "        return lm_logits, cls_logits"
      ],
      "metadata": {
        "id": "ORwuPmc2n5SA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s5p6YOne8Dbd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}